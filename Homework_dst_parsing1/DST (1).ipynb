{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#импортируем библиотеки\n",
    "import requests #чтобы получать код страница сайта\n",
    "from bs4 import BeautifulSoup #чтобы \"ходить\" по тэгам,доставать нужную нам информацию\n",
    "import pandas as pd #для работы с таблицами и файлами\n",
    "from time import sleep #для выставления задержек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#тут мы парсим кинопоиск, а именно результаты по запросу \"секс\". \n",
    "#наша задача собрать все названия фильмов со словом \"секс\" на русском языке, на языке оригинала, год выхода фильма и страну\n",
    "\n",
    "\n",
    "data = [] #создаем список для хранения наших данных\n",
    "\n",
    "for i in range(1,7): #тут прописываем сколько ссылок нужно пройти. в нашем случае их шесть (плюс одна) и начинаем мы с 1\n",
    "    url = 'https://www.kinopoisk.ru/s/type/film/list/1/find/%F1%E5%EA%F1/order/relevant/page/'+ str(i)+'/' #генерируем нашу ссылку на страницу\n",
    "    r = requests.get(url) #делаем запрос к сайту\n",
    "    sleep(0.5) #ставим задержку в половину секунды, чтобы не слишком мучить сайт\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'lxml') #если все хорошо, то мы получаем в ответ код страницы и передаем его нашему супу\n",
    "\n",
    "    films = soup.findAll('div', class_='element') #находим все теги, в которых лежит инфа по фильмам, получаем их в виде списка\n",
    "\n",
    "    for f in films: #начинаем перебирать эти теги и уточнять, что именно из них нам нужно достать\n",
    "        russian_name = f.find('div', class_='info').find('p').find('a').text #достаем имя на русском\n",
    "        #print(russian_name)\n",
    "\n",
    "        origin_name = f.find('div', class_='info').find('span', class_='gray').text #достаем оригинальное имя\n",
    "        #print(origin_name)\n",
    "\n",
    "        year = f.find('div', class_='info').find('span').text #достаем год\n",
    "        #print(year)\n",
    "\n",
    "        country = f.find('div', class_='info').findAll('span', class_='gray')[1].text.split(',')[0] #достаем страну произдводства\n",
    "        #print(country)\n",
    "        #print('-----')\n",
    "\n",
    "        data.append([russian_name, origin_name, year, country]) #записываем все наши данные в список\n",
    "\n",
    "              \n",
    "    \n",
    "header = ['russian_name', 'origin_name', 'year', 'country'] #тут у нас лежат названия наших колонок\n",
    "#а ниже просто сохраняем наш список в файл\n",
    "df = pd.DataFrame(data, columns=header)   \n",
    "df.to_csv('ТУТ ВАША ДИРЕКТОРИЯ/kinopoisk_data.csv', sep=';', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#это просто еще один метод сохранения файлов с помощью библиотеки csv (для этого нужно импортировать ее вверху import csv)\n",
    "import csv \n",
    "\n",
    "csv_file = open('ТУТ ВАША ДИРЕКТОРИЯ\\ПУТЬ', 'w')\n",
    "writer = csv.writer(csv_file, delimiter=';')\n",
    "writer.writerow(head)\n",
    "\n",
    "for row in data:\n",
    "    writer.writerow(row)\n",
    "\n",
    "csv_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#а тут мы парсим страничку кинопоиска ТОР-250 фильмов, собираем название фильма, год и ссылку на страничку фильма\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    url = 'https://www.kinopoisk.ru/lists/top250/?page=' + str(i) + '&tab=all'\n",
    "    r = requests.get(url)\n",
    "    sleep(5) #тут можно увеличивать задержку, смотря как кинопоиск будет вас банить\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "    films = soup.findAll('div', class_='desktop-rating-selection-film-item')\n",
    "\n",
    "    for f in films:\n",
    "        name = f.find('div', class_='selection-film-item-meta selection-film-item-meta_theme_desktop').find('p').text\n",
    "        year = f.find('div', class_='selection-film-item-meta selection-film-item-meta_theme_desktop').find('p', class_='selection-film-item-meta__original-name').text\n",
    "\n",
    "        link = 'https://www.kinopoisk.ru'+f.find('a', class_='selection-film-item-meta__link').get('href') # вот тут мы получаем ссылку на страничку фильм. ищем тэг а, а потом с помощью команды get достаем href\n",
    "\n",
    "\n",
    "        data.append([name, year, link])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#а здесь мы ходим по нашему списку, берем из него ссылку на каждую страницу фильма, открываем эту страницу и достаем оценку кинопоиска и imdb\n",
    "new_data = [] #создадим новый пустой список, куда добавим уже спарсенный данные name, year, link + добавим новые kinopoisk_rate, imdb_rate\n",
    "for i in data:\n",
    "    link = i[2]\n",
    "    r = requests.get(link)\n",
    "    sleep(25)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    kinopoisk_rate = soup.find('a', class_='film-rating-value styles_rootPositive__ac3xv styles_rootLink__1CSPc').text\n",
    "    imdb_rate = soup.find('span', class_='styles_valueSection__19woS').text.replace('IMDb: ', '')\n",
    "    \n",
    "    print(i[0])\n",
    "    print(kinopoisk_rate)\n",
    "    print(imdb_rate)\n",
    "    print('-----')\n",
    "    \n",
    "    row = [i[0], i[1], i[2], kinopoisk_rate, imdb_rate]\n",
    "    new_data.append(row)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
